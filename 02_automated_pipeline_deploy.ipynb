{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15dd06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "# Authors: \n",
    "# Fabian Hirschmann <fhirschmann@google.com>, \n",
    "# Elia Secchi <eliasecchi@google.com>,\n",
    "# Megha Agarwal <meghaag@google.com>,\n",
    "# Mandie Quartly <mandieq@google.com>,\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70205c",
   "metadata": {},
   "source": [
    "# Automated MLOps pipeline build, testing and deployment\n",
    "\n",
    "In the previous notebook, you created a machine learning pipeline to train a model. In this session, it's all about automating the training and deployment of this model. Hence, the objective this notebook is to:\n",
    "\n",
    "1. Refactor your Kubeflow pipeline into a Python file that can be compiled into YAML in an automated fashion.\n",
    "1. Write a script to deploy a compiled Kubeflow pipeline to Vertex AI.\n",
    "1. Use Cloud Build (CI/CD) to compile, test, and run your Kubeflow pipeline.\n",
    "1. Create a Cloud Source Repository (Git) to automatically trigger Cloud Build on every change on the master branch\n",
    "1. [Optional] Create a [pipeline template](https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template) to allow for the pipeline to be reused and retriggered\n",
    "1. [Optional] Setup a schedule for the pipeline, which can be done in 2 ways:\n",
    "      - using Cloud Scheduler job, which sends a message to a Pub/Sub topic, and then calls a Cloud Function to trigger the VertexAI pipeline.\n",
    "      - using the new Vertex AI Pipelines Schedules API\n",
    "      \n",
    "      \n",
    "As part of this notebook, you'll create the following files using the `%%writefile` directive.\n",
    "- `src/requirements.txt`: Python requirements file listing all dependencies.\n",
    "- `src/pipeline.py`: File containing your Kubeflow pipeline in Python and logic to compile the pipeline into YAML.\n",
    "- `src/create-pipeline-template.py`: Python script to create a Kubeflow Pipeline Template in Vertex AI.\n",
    "- `src/submit-pipeline.py`: Python script to submit pipeline to Vertex AI.\n",
    "- `src/cloudbuild.yml`: Cloud build pipeline to run your CI/CD process.\n",
    "- `src/tests/test_pipeline.py`: Unit test for pipeline\n",
    "- `cf-trigger/main.py`: Cloud Function to trigger your Kubeflow Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e533bcd",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26a4f0",
   "metadata": {},
   "source": [
    "Ensure your Compute Engine default service account `{PROJECT_NUMBER}-compute@developer.gserviceaccount.com` has following permissions enabled:\n",
    "1. `Vertex AI User`\n",
    "1. `Cloud Build Editor`\n",
    "1. `Artifact Registry Writer`\n",
    "1. `Source Repository Administrator`\n",
    "1. `Storage Object Creator`\n",
    "1. `Storage Object Viewer`\n",
    "\n",
    "Ensure your Cloud Build default service account `{PROJECT_NUMBER}-@cloudbuild.gserviceaccount.com` has following permissions enabled:\n",
    "1. `Service Account User`\n",
    "1. `Vertex AI User`\n",
    "\n",
    "Please note relevant IAM permissions can sometimes take time (up to 15 min) to trickle through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fcba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "669e8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/requirements.txt\n",
    "kfp==2.0.0b9\n",
    "pytest==7.2.0\n",
    "pytz==2022.7\n",
    "google-cloud-aiplatform==1.20.0\n",
    "google-api-core==2.10.2\n",
    "google-auth==1.35.0\n",
    "google-cloud-bigquery==1.20.0\n",
    "google-cloud-core==1.7.3\n",
    "google-cloud-resource-manager==1.6.3\n",
    "google-cloud-storage==2.2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf05ca5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp==2.0.0b9\n",
      "  Downloading kfp-2.0.0b9.tar.gz (351 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.1/351.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytest==7.2.0\n",
      "  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz==2022.7\n",
      "  Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-aiplatform==1.20.0\n",
      "  Downloading google_cloud_aiplatform-1.20.0-py2.py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-api-core==2.10.2\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth==1.35.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-bigquery==1.20.0\n",
      "  Downloading google_cloud_bigquery-1.20.0-py2.py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.0/155.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-core==1.7.3\n",
      "  Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
      "Collecting google-cloud-resource-manager==1.6.3\n",
      "  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-storage==2.2.1\n",
      "  Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.1/107.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp==2.0.0b9->-r src/requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==2.0.0b9->-r src/requirements.txt (line 1)) (8.1.3)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.16\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting kfp-server-api<3.0.0,>=2.0.0a0\n",
      "  Downloading kfp-server-api-2.0.0a6.tar.gz (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting kubernetes<24,>=8.0.0\n",
      "  Downloading kubernetes-23.6.0-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==2.0.0b9->-r src/requirements.txt (line 1)) (3.19.6)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.6/636.6 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp==2.0.0b9->-r src/requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==2.0.0b9->-r src/requirements.txt (line 1)) (2.2.1)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==2.0.0b9->-r src/requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from kfp==2.0.0b9->-r src/requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from kfp==2.0.0b9->-r src/requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest==7.2.0->-r src/requirements.txt (line 2)) (6.0.0)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest==7.2.0->-r src/requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest==7.2.0->-r src/requirements.txt (line 2)) (22.2.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest==7.2.0->-r src/requirements.txt (line 2)) (23.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.20.0->-r src/requirements.txt (line 4)) (1.34.0)\n",
      "Collecting packaging\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.20.0->-r src/requirements.txt (line 4)) (1.22.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core==2.10.2->-r src/requirements.txt (line 5)) (2.28.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core==2.10.2->-r src/requirements.txt (line 5)) (1.58.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.35.0->-r src/requirements.txt (line 6)) (66.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.35.0->-r src/requirements.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.35.0->-r src/requirements.txt (line 6)) (4.9)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.35.0->-r src/requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: google-resumable-media>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.20.0->-r src/requirements.txt (line 7)) (2.4.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager==1.6.3->-r src/requirements.txt (line 9)) (0.12.6)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.2/75.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core==2.10.2->-r src/requirements.txt (line 5)) (1.51.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core==2.10.2->-r src/requirements.txt (line 5)) (1.48.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=0.3.1->google-cloud-bigquery==1.20.0->-r src/requirements.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest==7.2.0->-r src/requirements.txt (line 2)) (3.11.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==2.0.0b9->-r src/requirements.txt (line 1)) (0.19.3)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<3.0.0,>=2.0.0a0->kfp==2.0.0b9->-r src/requirements.txt (line 1)) (1.26.14)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<3.0.0,>=2.0.0a0->kfp==2.0.0b9->-r src/requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<3.0.0,>=2.0.0a0->kfp==2.0.0b9->-r src/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<24,>=8.0.0->kfp==2.0.0b9->-r src/requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<24,>=8.0.0->kfp==2.0.0b9->-r src/requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest==7.2.0->-r src/requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth==1.35.0->-r src/requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==2.10.2->-r src/requirements.txt (line 5)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core==2.10.2->-r src/requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==2.0.0b9->-r src/requirements.txt (line 1)) (0.38.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<24,>=8.0.0->kfp==2.0.0b9->-r src/requirements.txt (line 1)) (3.2.2)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-2.0.0b9-py3-none-any.whl size=499551 sha256=d32099fed6885662df821961e22bd777e00546d027349da4771e9d46698e471d\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/82/60/ba/0d8bcf3ea722459cc1be3635374d8bc45a171d7b4bb3674a35\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=285e01c7485a24b88147a39e0b28e16f686d8cbca1b72d2aacc0d7e355a398df\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/20/e8/7b/003fc14f02f262dd4614aec55e41147c8012e3dad98c936b76\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-2.0.0a6-py3-none-any.whl size=104199 sha256=5ed48909d3c9fc8002eff4c8f87fc7bfa5856a7f4a372bda3f6bf8ae1ca64736\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/f3/76/79/7e55f10d0267812ee4b447be855b6b9678b1816f84f9401849\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22283 sha256=b9d946b90f19c55a05d2fcf530b6791c93790e0eee164835cdcc745bc69a576f\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/5a/4e/89/a4493443c6aadb8a38e9709610efc3bfafaea3a4108df4c112\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: pytz, wrapt, termcolor, strip-hints, PyYAML, packaging, kfp-pipeline-spec, iniconfig, exceptiongroup, docstring-parser, cachetools, requests-toolbelt, pluggy, kfp-server-api, jsonschema, google-auth, fire, Deprecated, pytest, kubernetes, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, kfp, google-cloud-aiplatform\n",
      "\u001b[33m  WARNING: The script strip-hints is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script jsonschema is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts py.test and pytest are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-deprecated and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.19.0 requires jsonschema>=4.17.3, but you have jsonschema 3.2.0 which is incompatible.\n",
      "google-auth-oauthlib 0.8.0 requires google-auth>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.10.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 PyYAML-5.4.1 cachetools-4.2.4 docstring-parser-0.15 exceptiongroup-1.1.0 fire-0.5.0 google-api-core-2.10.2 google-auth-1.35.0 google-cloud-aiplatform-1.20.0 google-cloud-bigquery-1.20.0 google-cloud-core-1.7.3 google-cloud-resource-manager-1.6.3 google-cloud-storage-2.2.1 iniconfig-2.0.0 jsonschema-3.2.0 kfp-2.0.0b9 kfp-pipeline-spec-0.1.16 kfp-server-api-2.0.0a6 kubernetes-23.6.0 packaging-21.3 pluggy-1.0.0 pytest-7.2.0 pytz-2022.7 requests-toolbelt-0.10.1 strip-hints-0.1.10 termcolor-2.2.0 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r src/requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dca0511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf90436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"dsg-mlops-testing\"  # @param {type:\"string\"}\n",
    "     \n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)\n",
    "\n",
    "!gcloud config set project {PROJECT_ID} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ddbb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\"\n",
    "    \n",
    "BUCKET_NAME = f\"mlops-2-{PROJECT_ID}\"\n",
    "EXPERIMENT_NAME = \"mlops-2-experiment\"\n",
    "PIPELINE_NAME = \"mlops-2-pipeline\"\n",
    "ENDPOINT_NAME = \"mlops-2-endpoint\"\n",
    "REPOSITORY_NAME = f\"mlops-2-{PROJECT_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c24e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://mlops-2-dsg-mlops-testing/...\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -c regional -l $REGION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce3c4c-cd58-45aa-9e74-eb341a116ac8",
   "metadata": {},
   "source": [
    "Using the cell below, ensure the following cloud service APIs are enabled for this lab:\n",
    "1. `Vertex AI API`\n",
    "1. `Cloud Build API`\n",
    "1. `Artifact Registry API`\n",
    "1. `Cloud Source Repositories API`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e2de0bd-0940-4eb5-82a8-8e254e21cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation \"operations/acf.p2-737187930776-4b95f7a5-6df1-4e52-9a8a-2ac458e6cb85\" finished successfully.\n",
      "Operation \"operations/acat.p2-737187930776-1cadc9bf-fd26-4c3b-9ba4-07b266224aec\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable cloudbuild.googleapis.com\n",
    "!gcloud services enable artifactregistry.googleapis.com\n",
    "!gcloud services enable sourcerepo.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a66b6f",
   "metadata": {},
   "source": [
    "## Automated MLOps pipeline creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84feaa",
   "metadata": {},
   "source": [
    "### Create a script containing your Vertex AI/Kubeflow Pipeline to compile the pipeline into `pipeline.yaml`\n",
    "\n",
    "> <font color='green'>**Task 1**</font>\n",
    ">\n",
    "> Create a Python script `src/pipeline.py` that creates a file name `pipeline.yaml` from the Kubeflow pipeline you developed last week. The output file should be in YAML and not JSON format.\n",
    ">\n",
    "> If you were unable to produce a Kubeflow pipeline last week, please use the one provided below. Otherwise, replace it with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d71ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/pipeline.py\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.dsl import component\n",
    "from kfp import compiler\n",
    "\n",
    "@component() \n",
    "def concat(a: str, b: str) -> str:\n",
    "    return a + b\n",
    "\n",
    "@component\n",
    "def reverse(a: str) -> NamedTuple(\"outputs\", [(\"before\", str), (\"after\", str)]):\n",
    "    return a, a[::-1]\n",
    "\n",
    "@pipeline(name=\"mlops-pipeline\")\n",
    "def basic_pipeline(a: str='stres', b: str='sed'):\n",
    "    concat_task = concat(a=a, b=b)\n",
    "    reverse_task = reverse(a=concat_task.output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    compiler.Compiler().compile(pipeline_func=basic_pipeline, package_path=\"pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d4148",
   "metadata": {},
   "source": [
    "Using the next command, you can test the materialized pipeline generated by your script. You can view the output in a file named `pipeline.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c94289",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66bb273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PIPELINE DEFINITION\n",
      "# Name: mlops-pipeline\n",
      "# Inputs:\n",
      "#    a: str [Default: 'stres']\n",
      "#    b: str [Default: 'sed']\n",
      "components:\n",
      "  comp-concat:\n",
      "    executorLabel: exec-concat\n",
      "    inputDefinitions:\n",
      "      parameters:\n",
      "        a:\n",
      "          parameterType: STRING\n",
      "        b:\n",
      "          parameterType: STRING\n",
      "    outputDefinitions:\n",
      "      parameters:\n",
      "        Output:\n",
      "          parameterType: STRING\n",
      "  comp-reverse:\n",
      "    executorLabel: exec-reverse\n"
     ]
    }
   ],
   "source": [
    "!head -n20 pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca25b67-c432-4d42-8a10-2b9b94e7a980",
   "metadata": {},
   "source": [
    "### Test the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef73fcf-6a9c-49da-bb65-7f724a412c5e",
   "metadata": {},
   "source": [
    "> <font color='green'>**Task 2**</font>\n",
    "> Write unit/integration tests for the pipeline you created to ensure the component logic that you added works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e162c6-1cbb-456c-8b5c-d513fbe34064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8186aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/tests/test_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/tests/test_pipeline.py\n",
    "\n",
    "import unittest\n",
    "from pipeline import concat, reverse, basic_pipeline\n",
    "\n",
    "class TestBasicPipeline(unittest.TestCase):\n",
    "    # def setUp(self):\n",
    "        # Get relevant component\n",
    "    \n",
    "    def test_concat_component(self):\n",
    "        self.assertEqual(concat.python_func(3, 3), 6)\n",
    "\n",
    "    def test_reverse(self):\n",
    "        self.assertEqual(reverse.python_func(\"stressed\")[1], \"desserts\")\n",
    "\n",
    "    def test_pipeline(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a7b41-9996-42c1-8849-70aef63273f9",
   "metadata": {},
   "source": [
    "Using the next command, you can run the tests in the script using python `unittest` test runner. It discovers all the test files that start with `test_*`\n",
    "\n",
    "You can also use other testing framework of your choice (e.g. `pytest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "191cfe48-0206-477c-a894-7c4ca66d6eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src python -m unittest discover -s src/tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a576513",
   "metadata": {},
   "source": [
    "### Create a script to submit your compile kubeflow pipeline (`pipeline.yaml`) to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c708bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/submit-pipeline.py\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import google.auth\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "if not PROJECT_ID:\n",
    "    creds, PROJECT_ID = google.auth.default()\n",
    "\n",
    "REGION = os.environ[\"REGION\"]\n",
    "BUCKET_NAME = os.environ[\"BUCKET_NAME\"]\n",
    "EXPERIMENT_NAME = os.environ[\"EXPERIMENT_NAME\"]\n",
    "ENDPOINT_NAME = os.environ[\"ENDPOINT_NAME\"]\n",
    "PIPELINE_NAME = os.environ[\"PIPELINE_NAME\"]\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "sync_pipeline = os.getenv(\"SUBMIT_PIPELINE_SYNC\", 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path='pipeline.yaml',\n",
    "    location=REGION,\n",
    "    project=PROJECT_ID,\n",
    "    enable_caching=True,\n",
    "    pipeline_root=f'gs://{BUCKET_NAME}'\n",
    ")\n",
    "print(f\"Submitting pipeline {PIPELINE_NAME} in experiment {EXPERIMENT_NAME}.\")\n",
    "job.submit(experiment=EXPERIMENT_NAME)\n",
    "\n",
    "if sync_pipeline:\n",
    "    job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3de2c1",
   "metadata": {},
   "source": [
    "Let's test this script in the Notebook. You can check the pipeline's status by clicking on the link printed by the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d630863",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%set_env REGION=$REGION\n",
    "%set_env BUCKET_NAME=$BUCKET_NAME\n",
    "%set_env EXPERIMENT_NAME=$EXPERIMENT_NAME\n",
    "%set_env PIPELINE_NAME=$PIPELINE_NAME\n",
    "%set_env ENDPOINT_NAME=$ENDPOINT_NAME\n",
    "%set_env SUBMIT_PIPELINE_SYNC=1\n",
    "\n",
    "!python src/submit-pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faaf9dd",
   "metadata": {},
   "source": [
    "### Create a pipeline template in Artifact Registry from `pipeline.yaml`\n",
    "\n",
    "A pipeline template is a resource that you can use to publish a workflow definition so that it can be reused multiple times, by a single user or by multiple users. This feature is [documented here](https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template).\n",
    "\n",
    "The Kubeflow Pipelines SDK registry client is a new client interface that you can use with a compatible registry server, such as Artifact Registry, for version control of your Kubeflow Pipelines (KFP) templates.\n",
    "\n",
    "> <font color='green'>**Task 4**</font>\n",
    ">\n",
    "> Create a Python script `src/create-pipeline-template.py` that uploads `pipeline.yaml` to the Vertex AI Pipeline registry.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create mlops2-repo --location=$REGION --repository-format=KFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/create-pipeline-template.py\n",
    "import os\n",
    "import google.auth\n",
    "\n",
    "from kfp.registry import RegistryClient\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "if not PROJECT_ID:\n",
    "    creds, PROJECT_ID = google.auth.default()\n",
    "REGION = os.environ[\"REGION\"]\n",
    "\n",
    "client = RegistryClient(host=f\"https://{REGION}-kfp.pkg.dev/{PROJECT_ID}/mlops2-repo\")\n",
    "\n",
    "template_name, template_version = client.upload_pipeline(\n",
    "  file_name=\"pipeline.yaml\",\n",
    "  tags=[\"v1\", \"latest\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/create-pipeline-template.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cf0be",
   "metadata": {},
   "source": [
    "### Automate Kubeflow pipeline compilation, template generation, and execution through Cloud Build\n",
    "\n",
    "Cloud Build is a service that executes your builds on Google Cloud. In this exercise, we want to use it to both compile and run your machine learning pipeline. For more information, please refer to the [Cloud Build documentation](https://cloud.google.com/build/docs/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243972cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/cloudbuild.yaml\n",
    "steps:\n",
    "  # Install dependencies\n",
    "  - name: 'python'\n",
    "    entrypoint: 'pip'\n",
    "    args: [\"install\", \"-r\", \"requirements.txt\", \"--user\"]\n",
    "\n",
    "  # Compile pipeline\n",
    "  - name: 'python'\n",
    "    entrypoint: 'python'\n",
    "    args: ['pipeline.py']\n",
    "    id: 'compile'\n",
    "\n",
    "  # Test the Pipeline Components \n",
    "  - name: 'python'\n",
    "    entrypoint: 'python'\n",
    "    args: ['-m', 'unittest', 'discover', 'tests/']\n",
    "    id: 'test_pipeline'\n",
    "    waitFor: ['compile']\n",
    "\n",
    "  # Upload compiled pipeline to GCS\n",
    "  - name: 'gcr.io/cloud-builders/gsutil'\n",
    "    args: ['cp', 'pipeline.yaml', 'gs://${_BUCKET_NAME}']\n",
    "    id: 'upload'\n",
    "    waitFor: ['test_pipeline']\n",
    "        \n",
    "  # Run the Vertex AI Pipeline (synchronously for test/qa environment).\n",
    "  - name: 'python'\n",
    "    id: 'test'\n",
    "    entrypoint: 'python'\n",
    "    env: ['BUCKET_NAME=${_BUCKET_NAME}', 'EXPERIMENT_NAME=qa-${_EXPERIMENT_NAME}', 'PIPELINE_NAME=${_PIPELINE_NAME}',\n",
    "          'REGION=${_REGION}', 'ENDPOINT_NAME=qa-${_ENDPOINT_NAME}', 'SUBMIT_PIPELINE_SYNC=true']\n",
    "    args: ['submit-pipeline.py']\n",
    "\n",
    "  # Create pipeline template and upload it to the artifact registry\n",
    "  - name: 'python'\n",
    "    id: 'template'\n",
    "    entrypoint: 'python'\n",
    "    env: ['REGION=${_REGION}']\n",
    "    args: ['create-pipeline-template.py']\n",
    "    \n",
    "  # Run the Vertex AI Pipeline (asynchronously for prod environment). In a real production scenario, this would run in a different GCP project.\n",
    "  - name: 'python'\n",
    "    id: 'prod'\n",
    "    entrypoint: 'python'\n",
    "    env: ['BUCKET_NAME=${_BUCKET_NAME}', 'EXPERIMENT_NAME=prod-${_EXPERIMENT_NAME}', 'PIPELINE_NAME=${_PIPELINE_NAME}',\n",
    "          'REGION=${_REGION}', 'ENDPOINT_NAME=prod-${_ENDPOINT_NAME}', 'SUBMIT_PIPELINE_SYNC=false']\n",
    "    args: ['submit-pipeline.py']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4958d76",
   "metadata": {},
   "source": [
    "Cloud Build uses a special service account to execute builds on your behalf. When you enable the Cloud Build API on a Google Cloud project, the Cloud Build service account is automatically created and granted the Cloud Build Service Account role for the project. This role gives the service account permissions to perform several tasks, however you can grant more permissions to the service account to perform additional tasks. [This page](https://cloud.google.com/build/docs/securing-builds/configure-access-for-cloud-build-service-account) explains how to grant and revoke permissions to the Cloud Build service account.\n",
    "\n",
    "For Cloud Build to be able to deploy your pipeline, you need to give its' service account `{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com` the **Vertex AI User** and **Service Account User** role. Note that it may take up to 5 minutes until the new permissions propagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3ea3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit ./src --config=src/cloudbuild.yaml --substitutions=_BUCKET_NAME=$BUCKET_NAME,_EXPERIMENT_NAME=$EXPERIMENT_NAME,_PIPELINE_NAME=$PIPELINE_NAME,_REGION=$REGION,_ENDPOINT_NAME=$ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68abf9",
   "metadata": {},
   "source": [
    "### Create a git repository and trigger Cloud Build execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3a221",
   "metadata": {},
   "source": [
    "Before you can create the repository here, please **enable Cloud Source Repositories API** in the Google Cloud console.\n",
    "\n",
    "> <font color='green'>**Task 5**</font>\n",
    ">\n",
    "> Create a build trigger on the source repository that executes `cloudbuild.yaml`. Make sure to pass all **--substitutions**.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953b256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud source repos create $REPOSITORY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!gcloud beta builds triggers create cloud-source-repositories \\\n",
    "    --name=mlops2-source-trigger \\\n",
    "    --repo=$REPOSITORY_NAME \\\n",
    "    --branch-pattern=master \\\n",
    "    --build-config=cloudbuild.yaml \\\n",
    "    --substitutions=_BUCKET_NAME=$BUCKET_NAME,_EXPERIMENT_NAME=$EXPERIMENT_NAME,_PIPELINE_NAME=$PIPELINE_NAME,_REGION=$REGION,_ENDPOINT_NAME=$ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud source repos clone $REPOSITORY_NAME --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fd771",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -av src/* $REPOSITORY_NAME/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd844c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $REPOSITORY_NAME && git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09baca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"{YOUR_EMAIl}\"\n",
    "!git config --global user.name \"{YOUR_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $REPOSITORY_NAME && git commit -a -m \"initial commit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $REPOSITORY_NAME && git push -u origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b8347",
   "metadata": {},
   "source": [
    "## Schedule pipeline execution\n",
    "\n",
    "> <font color='green'>**Task 6**</font>\n",
    ">\n",
    "> Schedule the execution of the pipeline such that it runs every day.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe7a9b",
   "metadata": {},
   "source": [
    "### Schedule Method 1: Cloud Scheduler → Pub/Sub → Cloud Functions\n",
    "\n",
    "Using this method, you create a Pub/Sub topic that triggers a Cloud Function that triggers a Vertex AI Pipeline. Note that the service account the Cloud Function runs as needs access to Cloud Storage and Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub topics create trigger-mlops-2-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p cf-trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cf-trigger/requirements.txt\n",
    "kfp==2.0.0b9\n",
    "pytest==7.2.0\n",
    "google-cloud-aiplatform==1.20.0\n",
    "pytz==2022.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cf-trigger/main.py\n",
    "\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "REGION = os.environ[\"REGION\"]\n",
    "PIPELINE_NAME = os.environ[\"PIPELINE_NAME\"]\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "BUCKET_NAME = os.environ[\"BUCKET_NAME\"]\n",
    "\n",
    "\n",
    "def subscribe(event, context):\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    \n",
    "    job = aiplatform.PipelineJob(\n",
    "        display_name=PIPELINE_NAME,\n",
    "        template_path=f'gs://{BUCKET_NAME}/pipeline.yaml',\n",
    "        location=REGION,\n",
    "        project=PROJECT_ID,\n",
    "        enable_caching=False,\n",
    "        pipeline_root=f'gs://{BUCKET_NAME}'\n",
    "    )\n",
    "\n",
    "    job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b053fa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud functions deploy mlops-2-function \\\n",
    "--source=./cf-trigger \\\n",
    "--entry-point=subscribe \\\n",
    "--trigger-topic trigger-mlops-2-pipeline \\\n",
    "--runtime python37 \\\n",
    "--ingress-settings internal-and-gclb \\\n",
    "--set-env-vars REGION=$REGION,PIPELINE_NAME=$PIPELINE_NAME,PROJECT_ID=$PROJECT_ID,BUCKET_NAME=$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "!gcloud scheduler jobs create pubsub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2113e3",
   "metadata": {},
   "source": [
    "### Schedule Method 2: Vertex AI Scheduler (in preview)\n",
    "\n",
    "To use the Schedules API during the Private Preview release, your project ID must be allowlisted in the `SCHEDULED_RUNS_TRUSTED_TESTER` group, you can use projects that you previously signed up with—for example. If you’re not in the trusted testers group, sign up by using this [sign-up form](https://docs.google.com/forms/d/e/1FAIpQLScDxABxIvqjeM_279dwTMmVfFBJD7qmW2leyU_ZBTYutJ62uA/viewform?usp=sf_link). After signing up, wait for a confirmation from the Vertex AI Pipelines team before continuing. The documentaion for the scheduler is [available here](https://docs.google.com/document/d/1PKHANJsxrK-6vcgM75NnkmNOri1m5Re7BFxb8brezRk/edit#heading=h.3momfsgqwp3z).\n",
    "\n",
    "The Vertex AI Schedule Service API is a new resource that lets you schedule ad hoc or recurring Vertex AI Pipeline runs. The goal of this service is to make scheduling a one off or recurring pipeline run simple, such that any Vertex AI user can quickly understand and leverage schedules to implement naive continuous training in their business.\n",
    "\n",
    "This new API will replace our previous guidance of using Cloud Scheduler with Cloud Functions to schedule a pipeline run. It may support additional Vertex AI resources in the future.\n",
    "\n",
    "Note that **Scheduler is currently only available via its REST interface and not the Vertex AI SDK**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f29797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import json\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path='pipeline.yaml',\n",
    "    location=REGION,\n",
    "    project=PROJECT_ID,\n",
    "    pipeline_root=f'gs://{BUCKET_NAME}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ec59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_TEMPLATE = job.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "token = creds.token\n",
    "service_account_email = creds.service_account_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59bf5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/schedules\"\n",
    "headers = {\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "\n",
    "## Your code goes below this line\n",
    "\n",
    "payload = {\n",
    "    \"display_name\": \"mlops2-schedule\",\n",
    "    \"cron\": \"0 1 * * *\",\n",
    "    \"max_concurrent_run_count\": \"2\",\n",
    "}\n",
    "\n",
    "print(requests.post(endpoint, json=payload, headers=headers).json())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
